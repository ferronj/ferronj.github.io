---
draft: true 
date: 2023-01-31 
#authors:
categories:
    - Projects
    - Machine Learning
    - Reinforcement Learning
---

### An Assortment of  notes converning LLMs other large models...


> These are notes from George Hotz's YouTube video on Q*, putting it here for convenience as I run out the door and for similarity in the model training space.

> [Q* YouTube Video](https://www.youtube.com/watch?v=2QO3vzwHXhg)

> [Tinygrad: A Minimalistic Deep Learning Library](https://tinygrad.org/)
> [Tinygrad GitHub Repository](https://github.com/tinygrad/tinygrad)

> [Mistral-7B Model by Mistral AI](https://huggingface.co/mistralai/Mistral-7B-v0.1)
> [OpenHermes-2-Mistral-7B Model by Teknium](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B)




Ok...the Pokémon Red training. Very interesting, looks like a fun way to learn how to use either external compute or just the complexity of training a large model, whereas the Q* YouTube video shows an example of implementing an existing training set and modifying it for a specific purpose.

- Pokémon Red Training: [GitHub Repository](https://github.com/PWhiddy/PokemonRedExperiments)
- Stable Baseline 3: [GitHub Repository](https://github.com/DLR-RM/stable-baselines3)

I think another interesting application will be to create my own "assistant" using one of these local models and run it through a web app (Flask probably, or Streamlit) accessed through this website, thinking about support for analyzing research papers.

Other notes for later:

- NEAT-Python: [Documentation](https://neat-python.readthedocs.io/en/latest/)
- DEAP: [Documentation](https://deap.readthedocs.io/en/master/)
